{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Executable Notebook\r\n",
    "\r\n",
    "This notebook trains a number of models on the NOAA active sunspot region data.\r\n",
    "\r\n",
    "There is functionality at the end of the notebook to read sunspot region data from a standard docx file and produce predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from docx2python import docx2python\r\n",
    "from imblearn.over_sampling import SMOTE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Import data\r\n",
    "\r\n",
    "raw_df = pd.read_csv('SRS_all_2001-2020_2.csv')\r\n",
    "raw_flares = pd.read_csv('flare_list_2.csv')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#Drop duplicate flares. This sorts the flares by class alphabetically and keeps the last value, so if both classes of flare occur the X occurence is retained.\r\n",
    "\r\n",
    "raw_flares.sort_values('goes_class_ind')\r\n",
    "raw_flares.drop_duplicates(subset=['ID'], inplace=True, keep='last')\r\n",
    "raw_flares.reset_index(inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#This merges the sunspot dataframe with the flare dataframe, matching on the ID (formed in Excel) representing date and location.\r\n",
    "\r\n",
    "merged_df = pd.merge(raw_df[['ID','Area','Z','Mag Type','Number of Sunspots','date']], raw_flares[['ID','goes_class_ind']], \r\n",
    "                     on = 'ID', \r\n",
    "                     how='left')\r\n",
    "\r\n",
    "#Replace any N/As with 0\r\n",
    "\r\n",
    "merged_df = merged_df.fillna(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Split McIntosh parameter into its constituent parts\r\n",
    "merged_df['Z-value'] = merged_df['Z'].str[0]\r\n",
    "merged_df['p-value'] = merged_df['Z'].str[1]\r\n",
    "merged_df['c-value'] = merged_df['Z'].str[2]\r\n",
    "\r\n",
    "#Create binary flare and class columns\r\n",
    "merged_df['X_Class'] = [1 if x == 'X' else 0 for x in merged_df['goes_class_ind']]\r\n",
    "merged_df['Binary_Flare'] = [1 if x == 'M' or x == 'X' else 0 for x in merged_df['goes_class_ind']]\r\n",
    "\r\n",
    "#Rename and drop columns as appropriate\r\n",
    "merged_df.rename(columns={'Number of Sunspots':'Number_of_Sunspots'}, inplace=True)\r\n",
    "merged_df.rename(columns={'Mag Type':'Mag_Type'}, inplace=True)\r\n",
    "\r\n",
    "merged_df.drop('Z', axis=1, inplace=True)\r\n",
    "merged_df.drop('goes_class_ind', axis=1, inplace=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#We need to remove all non-occurences where a flare without a location has occurred in the following 24 hours\r\n",
    "#We can use the raw flares list, comparing the full ID to the last 5 digits - all should have the full number of digits as the SRS data does.\r\n",
    "\r\n",
    "merged_df['ID_str'] = merged_df['ID'].astype(str)\r\n",
    "merged_df['ID_str'] = merged_df['ID_str'].str[-5:]\r\n",
    "merged_df['ID_str'] = merged_df['ID_str'].astype(int)\r\n",
    "\r\n",
    "raw_flares = raw_flares.fillna(0)\r\n",
    "raw_flares['ID2'] = raw_flares['ID'].astype(int)\r\n",
    "\r\n",
    "#Merge lists based on this new ID, which matches every event with a flare on that day if there was one\r\n",
    "merged_df = pd.merge(merged_df, raw_flares[['ID2']], \r\n",
    "                     left_on = 'ID_str',\r\n",
    "                     right_on = 'ID2', \r\n",
    "                     how='left')\r\n",
    "\r\n",
    "#Drop all non-flares that now have an entry in the new ID column (i.e. they have an unlocated flare on the same day)\r\n",
    "merged_df = merged_df.drop(merged_df[(merged_df.Binary_Flare == 0) & (merged_df.ID2 > 0)].index)\r\n",
    "\r\n",
    "#Finally, remove temporary ID columns\r\n",
    "merged_df.drop('ID_str', axis=1, inplace=True)\r\n",
    "merged_df.drop('ID2', axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#Define dictionaries for encoding\r\n",
    "\r\n",
    "Z_values = {'A' : 0.1, 'H' : 0.15, 'B' : 0.3, 'C' : 0.45, 'D' : 0.6, 'E' : 0.75, 'F' : 0.9}\r\n",
    "p_values = {'x' : 0, 'r' : 0.1, 's' : 0.3, 'a' : 0.5, 'h' : 0.7, 'k' : 0.9}\r\n",
    "c_values = {'x' : 0, 'o' : 0.1, 'i' : 0.5, 'c' : 0.9}\r\n",
    "Mag_values = {'Alpha' : 0, 'Beta' : 0.1, 'Gamma' : 0.5, 'Gamma-Delta' : 0.5, 'Beta-Gamma' : 0.3, 'Beta-Delta' : 0.6, 'Beta-Gamma-Delta' : 0.9,\r\n",
    "                'Beta-Gamma.' : 0.3, 'Beta-gamma' : 0.3, 'Beta-delta' : 0.3}\r\n",
    "\r\n",
    "#Map dataframe dictionary values\r\n",
    "\r\n",
    "merged_df['Z-value'] = merged_df['Z-value'].map(Z_values)\r\n",
    "merged_df['c-value'] = merged_df['c-value'].map(c_values)\r\n",
    "merged_df['p-value'] = merged_df['p-value'].map(p_values)\r\n",
    "merged_df['Mag_Type'] = merged_df['Mag_Type'].map(Mag_values)\r\n",
    "                "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#Create copy of dataframe and remove any columns/features that we don't want to include. Uncomment column headings (after the first three) to remove the named features.\r\n",
    "\r\n",
    "cleaned_df = merged_df.copy()\r\n",
    "\r\n",
    "cleaned_df.pop('ID')\r\n",
    "cleaned_df.pop('X_Class')\r\n",
    "cleaned_df.pop('date')\r\n",
    "\r\n",
    "#cleaned_df.pop('Mag_Type')\r\n",
    "#cleaned_df.pop('Number_of_Sunspots')\r\n",
    "#cleaned_df.pop('Area')\r\n",
    "#cleaned_df.pop('Z-value')\r\n",
    "#cleaned_df.pop('p-value')\r\n",
    "#cleaned_df.pop('c-value')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0        01/01/2001\n",
       "1        01/01/2001\n",
       "2        01/01/2001\n",
       "3        01/01/2001\n",
       "4        01/01/2001\n",
       "            ...    \n",
       "24534    29/12/2020\n",
       "24535    30/12/2020\n",
       "24536    30/12/2020\n",
       "24537    31/12/2020\n",
       "24538    31/12/2020\n",
       "Name: date, Length: 23461, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#Create feature and label vectors for training algorithms.\r\n",
    "\r\n",
    "train_df = cleaned_df\r\n",
    "scaler = StandardScaler()\r\n",
    "train_labels = np.array(train_df.pop('Binary_Flare'))\r\n",
    "train_features = np.array(train_df)\r\n",
    "train_features = scaler.fit_transform(train_features)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OPTIONAL:\r\n",
    "\r\n",
    "Run the following code block to over-sample the minority class. Set rs_count to the number you wish to multiply the minority class by. For example, with rs_count = 6, we are multiplying the number of flaring datapoints by 6."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "rs_count = 6\r\n",
    "\r\n",
    "Neg_resample = np.bincount(train_labels)[0]\r\n",
    "Pos_resample = rs_count*np.bincount(train_labels)[1]\r\n",
    "\r\n",
    "Class_rebalance = {0:Neg_resample, 1:Pos_resample}\r\n",
    "\r\n",
    "oversample = SMOTE(sampling_strategy=Class_rebalance)\r\n",
    "\r\n",
    "train_features, train_labels = oversample.fit_resample(train_features, train_labels)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CONTINUE:\r\n",
    "\r\n",
    "Whether re-sampled or not, code may be identically continued from this point onwards."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Define models and fit to training data. Any hyperparameter changes must be implemented here. Max_iter may need to be increased if a model does not converge.\r\n",
    "\r\n",
    "MLP_Model = MLPClassifier()                           \r\n",
    "RF_Model = RandomForestClassifier()                    \r\n",
    "Log_Model = LogisticRegression()\r\n",
    "\r\n",
    "MLP_Model.fit(train_features, train_labels)\r\n",
    "RF_Model.fit(train_features, train_labels)\r\n",
    "Log_Model.fit(train_features, train_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#Define and compile TensorFlow deep learning model. Input shape must be number of features used. Hyperparameter changes for this model should be implemented here.\r\n",
    "\r\n",
    "model = tf.keras.Sequential([\r\n",
    "    tf.keras.layers.Dense(100, activation='relu', input_shape=(6,)),\r\n",
    "    tf.keras.layers.Dense(2)\r\n",
    "])\r\n",
    "\r\n",
    "model.compile(optimizer='adam',\r\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
    "              )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#Fit TensorFlow model to training data. Number of epochs can be altered based on performnce.\r\n",
    "\r\n",
    "model.fit(train_features, train_labels, epochs=20, verbose=0)\r\n",
    "\r\n",
    "#Create probabilistic predictions for TensorFlow model.\r\n",
    "\r\n",
    "probability_model = tf.keras.Sequential([model, \r\n",
    "                                         tf.keras.layers.Softmax()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making a prediction:\r\n",
    "\r\n",
    "By this point we have trained the models on a large set of NOAA data. The code has been designed to make a prediction based on a single docx file. If the formatting of the docx file is changed, the code will not run."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#The docx file must be saved in the current working directory. Ensure that the '.docx' extension is used. One has been left in as an example.\r\n",
    "#This creates a dataframe with each row representing an active sunspot region.\r\n",
    "\r\n",
    "SRS_df = pd.DataFrame(docx2python('Space_Weather_Sunspot_Region_Summary_0200_2016-01-02T03-02-06Z.docx').body[1])\r\n",
    "SRS_df = SRS_df.iloc[1:,:]\r\n",
    "SRS_df = SRS_df.iloc[:-2,:]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#Rename columns and drop unnecessary ones\r\n",
    "SRS_df.columns = ['No', 'Loc', 'Lo', 'Area', 'Z', 'LL', 'NN', 'Mag_Type', 'Growth', 'M', 'X', 'P']\r\n",
    "\r\n",
    "SRS_df.drop('Loc', axis=1, inplace=True)\r\n",
    "SRS_df.drop('Lo', axis=1, inplace=True)\r\n",
    "SRS_df.drop('LL', axis=1, inplace=True)\r\n",
    "SRS_df.drop('Growth', axis=1, inplace=True)\r\n",
    "SRS_df.drop('M', axis=1, inplace=True)\r\n",
    "SRS_df.drop('X', axis=1, inplace=True)\r\n",
    "SRS_df.drop('P', axis=1, inplace=True)\r\n",
    "\r\n",
    "#We can see that each entry is formatted as a list. This can be easily rectified by mapping a function to the dataframe.\r\n",
    "def unlist(x):\r\n",
    "    if isinstance(x, list):\r\n",
    "        return x[0]\r\n",
    "    else:\r\n",
    "        return x\r\n",
    "\r\n",
    "SRS_df = SRS_df.applymap(unlist)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#Encode columns as before.\r\n",
    "\r\n",
    "#Split Z parameter\r\n",
    "SRS_df['Z-value'] = SRS_df['Z'].str[0]\r\n",
    "SRS_df['p-value'] = SRS_df['Z'].str[1]\r\n",
    "SRS_df['c-value'] = SRS_df['Z'].str[2]\r\n",
    "\r\n",
    "#Encode\r\n",
    "SRS_df['Z-value'] = SRS_df['Z-value'].map(Z_values)\r\n",
    "SRS_df['c-value'] = SRS_df['c-value'].map(c_values)\r\n",
    "SRS_df['p-value'] = SRS_df['p-value'].map(p_values)\r\n",
    "SRS_df['Mag_Type'] = SRS_df['Mag_Type'].map(Mag_values)\r\n",
    "\r\n",
    "SRS_df = SRS_df.dropna()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#Now fix some formatting issues with non-numerical characters. \r\n",
    "\r\n",
    "# For all docx files tested with, this was enough to remove any non-numerical characters, however if there is a non-numerical character that is not fixed here that is present\r\n",
    "# in the file being tested, then the code will not run.\r\n",
    "\r\n",
    "SRS_df = SRS_df[SRS_df['Area'] != '?']\r\n",
    "SRS_df = SRS_df[SRS_df['Area'] != '-']\r\n",
    "SRS_df['Area'] = SRS_df['Area'].map(lambda x: x.lstrip('~>').rstrip('*'))\r\n",
    "\r\n",
    "SRS_df['Area'] = SRS_df['Area'].astype(float)\r\n",
    "SRS_df['NN'] = SRS_df['NN'].astype(float)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#Create cleaned dataframe copy. If not all features have been used for training, they will need to be removed here.\r\n",
    "\r\n",
    "cleaned_SRS = SRS_df[['Area', 'Mag_Type', 'NN','Z-value', 'p-value', 'c-value']]\r\n",
    "\r\n",
    "#Scale features\r\n",
    "\r\n",
    "SRS_features = np.array(cleaned_SRS)\r\n",
    "SRS_features = scaler.transform(SRS_features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#Make predictions for each model.\r\n",
    "\r\n",
    "MLP_Prediction = MLP_Model.predict_proba(SRS_features)\r\n",
    "RF_Prediction = RF_Model.predict_proba(SRS_features)\r\n",
    "LogisticReg_Prediction = Log_Model.predict_proba(SRS_features)\r\n",
    "TensorFlow_Prediction = probability_model.predict(SRS_features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The predictions\r\n",
    "\r\n",
    "Finally, we may produce predicted probabilities. I have just used the MLP and Logistic Regression here as I believe these to be our best two models. With slight adjustment you may also view the random forest and deep learning predictions. They have been set to show probabilities to three decimal places, but again this may be adjusted as suits.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "\r\n",
    "for sunspot in range(len(MLP_Prediction)):\r\n",
    "    print('Active Sunspot Region {} Predicted Flaring Probabilities (MLP and LR): {:.3%}, {:.3%}\\n'.format(sunspot+1, MLP_Prediction[sunspot][1], LogisticReg_Prediction[sunspot][1]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Active Sunspot Region 1 Predicted Flaring Probabilities (MLP and LR): 6.172%, 10.392%\n",
      "\n",
      "Active Sunspot Region 2 Predicted Flaring Probabilities (MLP and LR): 0.272%, 4.015%\n",
      "\n",
      "Active Sunspot Region 3 Predicted Flaring Probabilities (MLP and LR): 15.728%, 6.005%\n",
      "\n",
      "Active Sunspot Region 4 Predicted Flaring Probabilities (MLP and LR): 0.646%, 1.610%\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "66e4a4b7c121db4243309d1eb7f3fba70ad006b305f754897095e92fca8ac47b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}